{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8ae095-badb-4273-82fa-27d68785a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import geopandas as gpd\n",
    "from matplotlib.patches import Circle\n",
    "from matplotlib.collections import PatchCollection\n",
    "from shapely.geometry import Point\n",
    "from sklearn.cluster import DBSCAN\n",
    "from itertools import product\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib as mpl\n",
    "from matplotlib.collections import LineCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc35775-0d49-4b45-8f77-a202bcf8b305",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485dc560-c041-49b4-b92e-6dad8e6b2535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the aggregation, merge, and melt\n",
    "def prepare_data_disaster(buyouts, disaster_aid, var, agg_func, value_name, group='disasterNumber'):\n",
    "    buyouts_agg = buyouts.groupby(group)[var].agg(agg_func).reset_index(name='Buyouts')\n",
    "    disaster_aid_agg = disaster_aid.groupby(group)[var].agg(agg_func).reset_index(name='Disaster\\nAid')\n",
    "    merged_data = pd.merge(buyouts_agg, disaster_aid_agg, on=group, how='inner')\n",
    "    melted_data = pd.melt(merged_data, \n",
    "                          id_vars=group, \n",
    "                          value_vars=['Disaster\\nAid', 'Buyouts'],  # Ensure Aid comes first\n",
    "                          var_name='Dataset', \n",
    "                          value_name=value_name)\n",
    "    return melted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb9fee33-b143-4bd7-9c14-d32b0af1eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare the data for plotting\n",
    "def split_and_prepare_data(claims_clusters, aid_expanded_data, buyouts, var, optimal_cluster):\n",
    "    # Add a Cluster_Group column to each dataset\n",
    "    for df in [claims_clusters, aid_expanded_data, buyouts]:\n",
    "        df['Cluster_Group'] = df[optimal_cluster].apply(lambda x: 'Unclustered' if x == -1 else 'Clustered')\n",
    "        df['Dataset'] = df.name  # Assign dataset names for identification\n",
    "    return claims_clusters, aid_expanded_data, buyouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e93d577-b8b1-4ef5-86ff-0cfe333459f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the aggregation, merge, and melt\n",
    "def prepare_data(buyouts, disaster_aid, claims_clusters, var, agg_func, value_name, group='disasterNumber'):\n",
    "    buyouts_agg = buyouts.groupby(group)[var].agg(agg_func).reset_index(name='Buyouts')\n",
    "    disaster_aid_agg = disaster_aid.groupby(group)[var].agg(agg_func).reset_index(name='Disaster\\nAid')\n",
    "    claims_clusters_agg = claims_clusters.groupby(group)[var].agg(agg_func).reset_index(name='Claims')\n",
    "    \n",
    "    # Merge all datasets\n",
    "    merged_data = pd.merge(buyouts_agg, disaster_aid_agg, on=group, how='inner')\n",
    "    merged_data = pd.merge(merged_data, claims_clusters_agg, on=group, how='inner')\n",
    "    \n",
    "    # Melt data for plotting\n",
    "    melted_data = pd.melt(merged_data, \n",
    "                          id_vars=group, \n",
    "                          value_vars=['Claims', 'Disaster\\nAid', 'Buyouts'],  # Ensure correct order\n",
    "                          var_name='Dataset', \n",
    "                          value_name=value_name)\n",
    "    return melted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3da62d64-2696-4dae-b23b-4efb81213a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size categories\n",
    "def classify_radius(size):\n",
    "    if size <= 15:\n",
    "        return 0.1  # Small size\n",
    "    elif 16 <= size <= 169:\n",
    "        return 0.3 # Medium size\n",
    "    else:\n",
    "        return 0.5  # Large size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48b771bd-8cad-45b8-b949-c45f427894c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep text editable in Illustrator & compress rasters\n",
    "mpl.rcParams.update({\n",
    "    \"pdf.fonttype\": 42,      # TrueType text\n",
    "    \"ps.fonttype\": 42,\n",
    "    \"pdf.compression\": 9,\n",
    "    \"path.simplify\": True,\n",
    "    \"path.simplify_threshold\": 0.3,\n",
    "})\n",
    "\n",
    "# Rasterize non-line collections on an axes (good for polygon fills, bars, boxes)\n",
    "def rasterize_non_lines(ax):\n",
    "    for coll in ax.collections:\n",
    "        coll.set_rasterized(True)\n",
    "        coll.set_zorder(0)\n",
    "    for patch in ax.patches:              # bars, box rectangles, violins\n",
    "        patch.set_rasterized(True)\n",
    "        patch.set_zorder(0)\n",
    "    for art in getattr(ax, \"artists\", []):  # seaborn box patches live here\n",
    "        if hasattr(art, \"set_rasterized\"):\n",
    "            art.set_rasterized(True)\n",
    "            art.set_zorder(0)\n",
    "    for line in ax.lines:                 # whiskers, caps, medians, hlines\n",
    "        line.set_rasterized(True)\n",
    "        line.set_zorder(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd7cb2-7057-4c37-a2fc-ebb658db0836",
   "metadata": {},
   "source": [
    "# Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81711fe0-9fa7-4432-a169-b5a659fe0ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'default'\n",
    "name = ''\n",
    "optimal_cluster = 'st_cluster_3_5_7'\n",
    "save = False\n",
    "\n",
    "if var == 'default':\n",
    "    var = 'returnPeriod_MSWEP_1d'\n",
    "    name = ''\n",
    "    folder = ''\n",
    "    optimal_cluster = 'st_cluster_3_5_7'\n",
    "else:\n",
    "    folder = 'SI/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8778350-c34a-4669-89cc-c92dd0c1e6ce",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "805f5949-7e87-4baa-bcab-f934fab0c393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11275/781343474.py:11: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  buyouts = pd.read_csv(\"../../../no_percentile_filter/final_filtered_buyouts.csv\")\n"
     ]
    }
   ],
   "source": [
    "disaster_aid_renters = pd.read_csv(\"../../../no_percentile_filter/final_filtered_aid_renters.csv\")\n",
    "disaster_aid_owners = pd.read_csv(\"../../../no_percentile_filter/final_filtered_aid_owners.csv\")\n",
    "\n",
    "# Add a column to identify the aid type\n",
    "disaster_aid_renters[\"aid_type\"] = \"Renter\"\n",
    "disaster_aid_owners[\"aid_type\"] = \"Owner\"\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "disaster_aid = pd.concat([disaster_aid_renters, disaster_aid_owners], axis=0, ignore_index=True)\n",
    "\n",
    "buyouts = pd.read_csv(\"../../../no_percentile_filter/final_filtered_buyouts.csv\")\n",
    "\n",
    "# Ensure that 'fullFIPS' is properly formatted as a 5-character string\n",
    "disaster_aid['fullFIPS'] = disaster_aid['fullFIPS'].astype(int).astype(str)\n",
    "disaster_aid['fullFIPS'] = disaster_aid['fullFIPS'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "# Ensure that 'countyCode' is properly formatted as a 5-character string\n",
    "buyouts['countyCode'] = buyouts['countyCode'].astype(int).astype(str)\n",
    "buyouts['countyCode'] = buyouts['countyCode'].apply(lambda x: str(x).zfill(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af2c900a-f938-4b0d-a291-f38ed2325595",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.read_csv('../../../Clusters/clustered_disasters_sensitivity.csv')\n",
    "# Ensure that 'fullFIPS' is properly formatted as a 5-character string\n",
    "clusters['fullFIPS'] = clusters['fullFIPS'].astype(int).astype(str)\n",
    "clusters['fullFIPS'] = clusters['fullFIPS'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "clusters[optimal_cluster]=clusters[optimal_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d8a4379-a174-4137-8c1c-e85c99c28d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the optimal_cluster column from 'clusters' into 'disaster_aid'\n",
    "disaster_aid = disaster_aid.merge(\n",
    "    clusters[['fullFIPS', 'disasterNumber', optimal_cluster]],\n",
    "    on=['fullFIPS', 'disasterNumber'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merging the optimal_cluster column from 'clusters' into 'buyouts'\n",
    "buyouts = buyouts.merge(\n",
    "    clusters[['fullFIPS', 'disasterNumber', optimal_cluster]],\n",
    "    left_on=['countyCode', 'disasterNumber'],\n",
    "    right_on=['fullFIPS', 'disasterNumber'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the GEOID column if you don't need it\n",
    "buyouts = buyouts.drop(columns=['fullFIPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75ca8f56-9b75-42fe-8d67-fb0a992d9abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11275/2256844461.py:2: DtypeWarning: Columns (8,26,33,36,39,43,52,53,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  claims_clusters = pd.read_csv('../../../Clusters/no_percentile_filter/clustered_claims_sensitivity.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load clustered claims\n",
    "claims_clusters = pd.read_csv('../../../Clusters/no_percentile_filter/clustered_claims_sensitivity.csv')\n",
    "\n",
    "# Ensure that 'countyCode' is properly formatted as a 5-character string\n",
    "claims_clusters['countyCode'] = claims_clusters['countyCode'].astype(int).astype(str)\n",
    "claims_clusters['countyCode'] = claims_clusters['countyCode'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "# Ensure 'dateOfLoss' in dt format\n",
    "claims_clusters['dateOfLoss'] = pd.to_datetime(claims_clusters['dateOfLoss'])  # Convert date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0de2dc1-c5b0-4fad-aa32-b4cd36a848ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data by expanding rows based on 'approvedForFemaAssistance'\n",
    "# Replicate each value by the count in 'approvedForFemaAssistance'\n",
    "aid_expanded_data = disaster_aid.loc[disaster_aid['approvedForFemaAssistance'] > 0].copy()\n",
    "aid_expanded_data = aid_expanded_data.loc[aid_expanded_data.index.repeat(aid_expanded_data['approvedForFemaAssistance'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1895af9-20e9-4252-8912-03ab4f36ba8a",
   "metadata": {},
   "source": [
    "# Using Disaster Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b2dea-4858-4c4a-b0c7-ef364f6c0886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th Percentile of cluster sizes: 137.40000000000055\n",
      "50th Percentile (Median) of cluster sizes: 15.0\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for the first and second plot (max and mean by storm cluster)\n",
    "melted_data_max = prepare_data(buyouts, aid_expanded_data, claims_clusters, var, 'max', 'Max Return Period (YR)', group=optimal_cluster)\n",
    "melted_data_mean = prepare_data(buyouts, aid_expanded_data, claims_clusters, var, 'mean', 'Mean Return Period (YR)', group=optimal_cluster)\n",
    "\n",
    "# Prepare the data for the second plot (clustered vs unclustered claims data)\n",
    "claims_clusters['Cluster_Group'] = claims_clusters[optimal_cluster].apply(lambda x: 'Unclustered' if x == -1 else 'Clustered')\n",
    "claims_clusters['Dataset'] = 'Claims'\n",
    "filtered_data = claims_clusters[claims_clusters['Cluster_Group'].isin(['Clustered', 'Unclustered'])]\n",
    "\n",
    "cluster_centers = claims_clusters.groupby(optimal_cluster).agg(\n",
    "    median_latitude=('latitude', 'median'),\n",
    "    median_longitude=('longitude', 'median'),\n",
    "    cluster_size=('latitude', 'size'),  # Count rows in each cluster\n",
    "    variable_max=(var, 'max'),  # Max of the variable `var` for color mapping\n",
    "    variable_mean=(var, 'mean')  # Mean of the variable `var` for color mapping\n",
    ").reset_index()\n",
    "\n",
    "# Sort by variable_max value to plot higher values last\n",
    "cluster_centers = cluster_centers.sort_values(by='variable_max', ascending=True)\n",
    "\n",
    "# Calculate the 90th and 50th percentiles of the cluster_size\n",
    "percentile_90 = np.percentile(cluster_centers['cluster_size'], 90)\n",
    "percentile_50 = np.percentile(cluster_centers['cluster_size'], 50)\n",
    "\n",
    "# Print the results\n",
    "print(f\"90th Percentile of cluster sizes: {percentile_90}\")\n",
    "print(f\"50th Percentile (Median) of cluster sizes: {percentile_50}\")\n",
    "\n",
    "# Classify cluster sizes into the three categories\n",
    "cluster_centers['radius'] = cluster_centers['cluster_size'].apply(classify_radius)\n",
    "\n",
    "# Load the county shapefile\n",
    "county_shapefile_path = '../../../../Local_Data/Geospatial/tl_2019_us_county.shp'\n",
    "gdf_counties = gpd.read_file(county_shapefile_path)\n",
    "\n",
    "# Load the shapefile for US states\n",
    "state_shapefile_path = '../../../../Local_Data/Geospatial/cb_2018_us_state_20m.shp'\n",
    "gdf_states = gpd.read_file(state_shapefile_path)\n",
    "\n",
    "# Set up figure dimensions\n",
    "fig_width = 7.08  # inches\n",
    "fig_height = fig_width*0.9 # Adjust aspect ratio for the layout\n",
    "\n",
    "# Create a 2x2 GridSpec layout\n",
    "fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "gs = GridSpec(2, 2, width_ratios=[3, 1])  # Maps wider than boxplots\n",
    "\n",
    "# Create axes for maps\n",
    "ax_map1 = fig.add_subplot(gs[0, 0])  # First map (Clustered Claims)\n",
    "ax_map2 = fig.add_subplot(gs[1, 0])  # Second map (Unclustered Claims)\n",
    "\n",
    "# Create axes for boxplots\n",
    "ax_box1 = fig.add_subplot(gs[0, 1])  # Boxplot for mean return period\n",
    "ax_box2 = fig.add_subplot(gs[1, 1])  # Boxplot for clustered vs unclustered\n",
    "\n",
    "# Define map extents (Contiguous US bounding box)\n",
    "extent = [-125, -66.5, 24, 49]  # [min_lon, max_lon, min_lat, max_lat]\n",
    "\n",
    "# Custom color palette for mapping\n",
    "colors = ['#007380', '#21BBBB', '#FFEFDB', '#ffcab7', '#ff9183']\n",
    "cmap_left = mcolors.LinearSegmentedColormap.from_list('custom_cmap', colors, N=256)\n",
    "norm = mcolors.LogNorm(vmin=1, vmax=100)  # Log normalization\n",
    "\n",
    "# Create a GeoDataFrame for cluster centers\n",
    "gdf_cluster_centers = gpd.GeoDataFrame(\n",
    "    cluster_centers,\n",
    "    geometry=gpd.points_from_xy(cluster_centers['median_longitude'], cluster_centers['median_latitude']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Plot the first map: Clustered Claims\n",
    "gdf_counties.plot(ax=ax_map1, color='lightgray', edgecolor='gray', alpha=0.5)\n",
    "patches = []\n",
    "for _, row in cluster_centers.iterrows():\n",
    "    center = (row['median_longitude'], row['median_latitude'])\n",
    "    radius = row['radius']\n",
    "    color = cmap_left(norm(row['variable_max'])) if not np.isnan(row['variable_max']) else 'gray'\n",
    "    circle = Circle(center, radius, edgecolor=color, facecolor=color, alpha=0.7)\n",
    "    patches.append(circle)\n",
    "p = PatchCollection(patches, match_original=True)\n",
    "ax_map1.add_collection(p)\n",
    "gdf_states.boundary.plot(ax=ax_map1, color='black', linewidth=0.5)\n",
    "\n",
    "# Rasterize heavy fills on the map only (keeps boundaries/text vector)\n",
    "rasterize_non_lines(ax_map1)\n",
    "ax_map1.set_xlim(extent[0], extent[1])\n",
    "ax_map1.set_ylim(extent[2], extent[3])\n",
    "ax_map1.axis('off')\n",
    "ax_map1.set_title(\"Cluster Max\", fontsize=12)\n",
    "\n",
    "# Add subplot label \"a)\" in the upper left\n",
    "ax_map1.text(-0.05, 1, \"a)\", transform=ax_map1.transAxes, fontsize=14, fontweight='bold', va='top', ha='left')\n",
    "\n",
    "# Plot the second map: Unclustered Claims\n",
    "gdf_counties.plot(ax=ax_map2, color='lightgray', edgecolor='gray', alpha=0.5)\n",
    "patches = []\n",
    "for _, row in cluster_centers.iterrows():\n",
    "    center = (row['median_longitude'], row['median_latitude'])\n",
    "    radius = row['radius']\n",
    "    color = cmap_left(norm(row['variable_mean'])) if not np.isnan(row['variable_mean']) else 'gray'\n",
    "    circle = Circle(center, radius, edgecolor=color, facecolor=color, alpha=0.7)\n",
    "    patches.append(circle)\n",
    "p = PatchCollection(patches, match_original=True)\n",
    "ax_map2.add_collection(p)\n",
    "gdf_states.boundary.plot(ax=ax_map2, color='black', linewidth=0.5)\n",
    "\n",
    "# Rasterize heavy fills on the map only (keeps boundaries/text vector)\n",
    "rasterize_non_lines(ax_map2)\n",
    "ax_map2.set_xlim(extent[0], extent[1])\n",
    "ax_map2.set_ylim(extent[2], extent[3])\n",
    "ax_map2.axis('off')\n",
    "ax_map2.set_title(\"Cluster Mean\", fontsize=12)\n",
    "\n",
    "# Add subplot label \"c)\" in the upper left\n",
    "ax_map2.text(-0.05, 1, \"c)\", transform=ax_map2.transAxes, fontsize=14, fontweight='bold', va='top', ha='left')\n",
    "\n",
    "# Add legend for point sizes with varying sizes\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='gray', markersize=3, label='Size ≤ 15', linestyle='None', alpha=0.5),\n",
    "    Line2D([0], [0], marker='o', color='gray', markersize=6, label='15 < Size < 170', linestyle='None', alpha=0.5),\n",
    "    Line2D([0], [0], marker='o', color='gray', markersize=9, label='Size ≥ 170', linestyle='None', alpha=0.5),\n",
    "]\n",
    "\n",
    "legend = ax_map1.legend(handles=legend_elements, loc='lower left', title='Claims per Cluster', frameon=True, fontsize=7)\n",
    "ax_map1.add_artist(legend)\n",
    "\n",
    "# Add horizontal colorbar below the second map\n",
    "cbar_ax = fig.add_axes([ax_map2.get_position().x0,  # Align with the left side of ax_map2\n",
    "                        ax_map2.get_position().y0 - 0.05,  # Slightly below ax_map2\n",
    "                        ax_map2.get_position().width,  # Match width of ax_map2\n",
    "                        0.02])  # Height of the colorbar\n",
    "\n",
    "# Create ScalarMappable for colorbar\n",
    "sm1 = plt.cm.ScalarMappable(cmap=cmap_left, norm=norm)\n",
    "cb1 = fig.colorbar(sm1, cax=cbar_ax, orientation='horizontal')\n",
    "cb1.ax.tick_params(labelsize=8)  # Adjust tick size\n",
    "cb1.set_label('Precipitation Return Period (Years)', fontsize=8)  # Label for colorbar\n",
    "\n",
    "# Plot the first boxplot: Mean Return Period\n",
    "sns.boxplot(\n",
    "    x='Dataset',\n",
    "    y='Max Return Period (YR)',\n",
    "    data=melted_data_max,\n",
    "    ax=ax_box1,\n",
    "    palette=[\"#babeee\", \"#90d2c3\", \"#ecf4be\"]\n",
    ")\n",
    "rasterize_non_lines(ax_box1)\n",
    "ax_box1.set_yscale('log')\n",
    "ax_box1.set_title('Max Return Period by Cluster')\n",
    "ax_box1.set_ylabel('Max Return Period (YR)')\n",
    "ax_box1.set_xlabel('')\n",
    "# Adjust x-axis label font size for the first boxplot\n",
    "ax_box1.set_xticklabels(ax_box1.get_xticklabels(), fontsize=8)  # Replace 8 with your desired font size\n",
    "\n",
    "medians_mean = melted_data_max.groupby('Dataset')['Max Return Period (YR)'].median()\n",
    "dataset_order = ['Claims', 'Disaster\\nAid', 'Buyouts']\n",
    "for i, dataset in enumerate(dataset_order):\n",
    "    median = medians_mean.get(dataset, None)\n",
    "    if median is not None:\n",
    "        ax_box1.text(i, median, f'{median:.1f}', ha='center', va='center', color='black')\n",
    "\n",
    "# Add subplot label \"b)\" in the upper left\n",
    "ax_box1.text(-0.37, 1, \"b)\", transform=ax_box1.transAxes, fontsize=14, fontweight='bold', va='top', ha='left')\n",
    "\n",
    "# Plot the second boxplot: Clustered vs Unclustered\n",
    "sns.boxplot(\n",
    "    x='Dataset',\n",
    "    y='Mean Return Period (YR)',\n",
    "    data=melted_data_mean,\n",
    "    ax=ax_box2,\n",
    "    palette=[\"#babeee\", \"#90d2c3\", \"#ecf4be\"]\n",
    ")\n",
    "rasterize_non_lines(ax_box2)\n",
    "ax_box2.set_yscale('log')\n",
    "ax_box2.set_title('Mean Return Period by Cluster')\n",
    "ax_box2.set_ylabel('Mean Return Period (YR)')\n",
    "ax_box2.set_xlabel('')\n",
    "# Adjust x-axis label font size for the first boxplot\n",
    "ax_box2.set_xticklabels(ax_box2.get_xticklabels(), fontsize=8)  # Replace 8 with your desired font size\n",
    "\n",
    "medians_mean = melted_data_mean.groupby('Dataset')['Mean Return Period (YR)'].median()\n",
    "dataset_order = ['Claims', 'Disaster\\nAid', 'Buyouts']\n",
    "for i, dataset in enumerate(dataset_order):\n",
    "    median = medians_mean.get(dataset, None)\n",
    "    if median is not None:\n",
    "        ax_box2.text(i, median, f'{median:.1f}', ha='center', va='center', color='black')\n",
    "\n",
    "# Add subplot label \"d)\" in the upper left\n",
    "ax_box2.text(-0.37, 1, \"d)\", transform=ax_box2.transAxes, fontsize=14, fontweight='bold', va='top', ha='left')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "if save:\n",
    "    plt.savefig('../../Plots/'+folder+'F6'+name+'.pdf',\n",
    "        format='pdf', dpi=500, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
