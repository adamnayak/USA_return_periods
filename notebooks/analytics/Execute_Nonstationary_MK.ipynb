{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157dac3c-83ec-48f2-b9d8-07bb766a3f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymannkendall\n",
    "import pymannkendall as mk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "from Nonstationary_MK import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2116a47e-2d30-4ac2-87bb-4d5b3408872c",
   "metadata": {},
   "source": [
    "# Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a96daa-4cd7-419b-a628-0171ce04ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'default'\n",
    "name = ''\n",
    "space_thres = 3 # Spatial threshold\n",
    "time_thres = 5  # Temporal threshold\n",
    "num_thres = 7 # Minimum number of points to form a cluster\n",
    "cluster = f'st_cluster_{space_thres}_{time_thres}_{num_thres}'\n",
    "\n",
    "save = True\n",
    "min_data = 10\n",
    "\n",
    "if var == 'default':\n",
    "    var = 'returnPeriod_MSWEP_1d'\n",
    "    name = '_MSWEP'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902472af-584b-42fe-be2b-5625fd9b39dc",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805eaf8-1c2e-44cf-8383-acd0e516b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined Processed_Claims.csv file\n",
    "processed_claims_file = \"../../no_percentile_filter/final_filtered_claims.csv\"\n",
    "processed_claims_df = pd.read_csv(processed_claims_file)\n",
    "\n",
    "# Ensure that 'countyCode' is properly formatted as a 5-character string\n",
    "processed_claims_df['countyCode'] = processed_claims_df['countyCode'].astype(int).astype(str)\n",
    "processed_claims_df['countyCode'] = processed_claims_df['countyCode'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "# Ensure 'dateOfLoss' in dt format\n",
    "processed_claims_df['dateOfLoss'] = pd.to_datetime(processed_claims_df['dateOfLoss'])  # Convert date column\n",
    "\n",
    "# Convert 'dateOfLoss' to ordinal (number of days since a fixed point)\n",
    "processed_claims_df['time'] = processed_claims_df['dateOfLoss'].map(pd.Timestamp.toordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b9596-7bdd-4e95-a7fa-02788e7e63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter counties with at least min unique data points\n",
    "claims = processed_claims_df.groupby('countyCode').filter(lambda g: g['time'].nunique() >= min_data and g[var].nunique() >= min_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917df79-5f83-4253-9c3d-e1fd393a857f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unified results storage\n",
    "results = {}\n",
    "\n",
    "for county, group in claims.groupby('countyCode'):\n",
    "    print(f\"Processing county: {county} for {name}...\")\n",
    "\n",
    "    group = group.sort_values('time')  # Ensure sorted by time\n",
    "    time_values = group['time'].values\n",
    "    var_values = group[var].values\n",
    "\n",
    "    # Skip counties with insufficient data\n",
    "    if len(time_values) < 2 or len(np.unique(time_values)) < 2:\n",
    "        print(f\"Skipping county: {county} for {name} (insufficient data)\")\n",
    "        continue\n",
    "\n",
    "    # Random sampling for large datasets\n",
    "    if len(var_values) > 75000:\n",
    "        print(f\"County {county} has {len(var_values)} records. Using random sampling...\")\n",
    "        mk_results = random_sampling_mk(var_values)\n",
    "    else:\n",
    "        try:\n",
    "            result = mk.original_test(var_values)\n",
    "            print('mk complete')\n",
    "            # Compute additional details\n",
    "            sample_size = len(var_values)\n",
    "            test_statistic = result.Tau  # Kendall's Tau correlation coefficient\n",
    "            effect_size = abs(result.Tau)  # Effect size (absolute Tau value)\n",
    "            confidence_interval = (result.slope - 1.96 * result.intercept, \n",
    "                                   result.slope + 1.96 * result.intercept)  # Approximate 95% CI\n",
    "            dof = sample_size - 1  # Degrees of Freedom (approximation)\n",
    "\n",
    "            mk_results = {\n",
    "                \"trend\": result.trend,\n",
    "                \"p-value\": result.p,\n",
    "                \"slope\": result.slope,\n",
    "                \"significant\": result.p < 0.05,\n",
    "                \"sample_size\": sample_size,\n",
    "                \"test_statistic\": test_statistic,\n",
    "                \"effect_size\": effect_size,\n",
    "                \"confidence_interval_lower\": confidence_interval[0],\n",
    "                \"confidence_interval_upper\": confidence_interval[1],\n",
    "                \"degrees_of_freedom\": dof\n",
    "            }\n",
    "            \n",
    "        except ZeroDivisionError:\n",
    "            print(f\"Skipping county: {county} for {name} (Mann-Kendall Test failed due to zero-division)\")\n",
    "            continue\n",
    "    \n",
    "    # Append results to unified results dictionary\n",
    "    if county not in results:\n",
    "        results[county] = {}\n",
    "\n",
    "    results[county][\"trend\"] = mk_results[\"trend\"]\n",
    "    results[county][\"p-value\"] = mk_results[\"p-value\"]\n",
    "    results[county][\"slope\"] = mk_results[\"slope\"] #sen_slope_value\n",
    "    results[county][\"significant\"] = mk_results[\"significant\"]\n",
    "    results[county][\"sample_size\"] = mk_results[\"sample_size\"]\n",
    "    results[county][\"test_statistic\"] = mk_results[\"test_statistic\"]\n",
    "    results[county][\"effect_size\"] = mk_results[\"effect_size\"]\n",
    "\n",
    "    print(f\"Finished processing county: {county} for {name}\")\n",
    "\n",
    "# Convert unified results to DataFrame\n",
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\").reset_index()\n",
    "results_df.rename(columns={\"index\": \"countyCode\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe34a79d-4a6e-4060-b682-7e82b925cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save:\n",
    "    results_df.to_csv(\"Nonstationary_MK/claim_trends\"+name+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b86b1e-9ba5-4e9d-87b0-1512d22ed773",
   "metadata": {},
   "source": [
    "# Load Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb4042-94c4-4c52-a567-5f20cc22aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_clusters = pd.read_csv('Clusters/no_percentile_filter/clustered_claims_sensitivity.csv')\n",
    "claims_clusters['countyCode'] = claims_clusters['countyCode'].astype(int).astype(str)\n",
    "claims_clusters['countyCode'] = claims_clusters['countyCode'].apply(lambda x: str(x).zfill(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3fc59-d705-4e66-b208-017bd749b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure `dateOfLoss` is in datetime format\n",
    "claims_clusters['dateOfLoss'] = pd.to_datetime(claims_clusters['dateOfLoss'])\n",
    "\n",
    "# Group by cluster to compute median date and cluster size\n",
    "cluster_summary = (\n",
    "    claims_clusters.groupby(cluster)\n",
    "    .agg(\n",
    "        median_dateOfLoss=('dateOfLoss', 'median'),\n",
    "        mean_returnPeriod=(var, 'mean'),\n",
    "        cluster_size=('dateOfLoss', 'size')\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40e8bf-8c9f-4b2c-b2ed-3841a3e8d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert median dateOfLoss to ordinal format for numeric trend analysis\n",
    "cluster_summary['ordinal_date'] = cluster_summary['median_dateOfLoss'].map(pd.Timestamp.toordinal)\n",
    "\n",
    "# Sort data by median date for time series analysis\n",
    "cluster_summary = cluster_summary.sort_values('ordinal_date')\n",
    "\n",
    "# Extract time series\n",
    "dates = cluster_summary['ordinal_date']\n",
    "sizes = cluster_summary['cluster_size']\n",
    "returns = cluster_summary['mean_returnPeriod']\n",
    "\n",
    "# Perform the Mann-Kendall Test\n",
    "mk_result_dates = mk.original_test(dates)\n",
    "\n",
    "# Compute additional details\n",
    "sample_size = len(dates)\n",
    "test_statistic = mk_result_dates.Tau  # Kendall's Tau\n",
    "effect_size = abs(mk_result_dates.Tau)  # Effect size in Kendall's Tau (absolute value)\n",
    "confidence_interval = (mk_result_dates.slope - 1.96 * mk_result_dates.intercept, \n",
    "                       mk_result_dates.slope + 1.96 * mk_result_dates.intercept)  # Approximate 95% CI\n",
    "dof = sample_size - 1  # Degrees of Freedom (approximation)\n",
    "\n",
    "# Print results\n",
    "print(\"Mann-Kendall Test for Median Date:\")\n",
    "print(f\"Trend: {mk_result_dates.trend}\")\n",
    "print(f\"P-value: {mk_result_dates.p}\")\n",
    "print(f\"Significance: {'Significant' if mk_result_dates.p < 0.05 else 'Not Significant'}\")\n",
    "print(f\"Sen's Slope for Median Date: {mk_result_dates.slope}\")\n",
    "print(f\"Sample Size: {sample_size}\")\n",
    "print(f\"Test Statistic (Tau): {test_statistic}\")\n",
    "print(f\"Effect Size: {effect_size}\")\n",
    "print(f\"Confidence Interval (Approximate 95% CI): {confidence_interval}\")\n",
    "print(f\"Degrees of Freedom (DOF): {dof}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73199a2c-647d-4e27-842d-4a9226108eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Mann-Kendall Test\n",
    "mk_result_sizes = mk.original_test(sizes)\n",
    "\n",
    "# Compute additional details\n",
    "sample_size = len(sizes)\n",
    "test_statistic = mk_result_sizes.Tau  # Kendall's Tau\n",
    "effect_size = abs(mk_result_sizes.Tau)  # Effect size in Kendall's Tau (absolute value)\n",
    "confidence_interval = (mk_result_sizes.slope - 1.96 * mk_result_sizes.intercept, \n",
    "                       mk_result_sizes.slope + 1.96 * mk_result_sizes.intercept)  # Approximate 95% CI\n",
    "dof = sample_size - 1  # Degrees of Freedom (approximation)\n",
    "\n",
    "# Print results\n",
    "print(\"Mann-Kendall Test for Cluster Size:\")\n",
    "print(f\"Trend: {mk_result_sizes.trend}\")\n",
    "print(f\"P-value: {mk_result_sizes.p}\")\n",
    "print(f\"Significance: {'Significant' if mk_result_sizes.p < 0.05 else 'Not Significant'}\")\n",
    "print(f\"Sen's Slope for Cluster Size: {mk_result_sizes.slope}\")\n",
    "print(f\"Sample Size: {sample_size}\")\n",
    "print(f\"Test Statistic (Tau): {test_statistic}\")\n",
    "print(f\"Effect Size: {effect_size}\")\n",
    "print(f\"Confidence Interval (Approximate 95% CI): {confidence_interval}\")\n",
    "print(f\"Degrees of Freedom (DOF): {dof}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75f001-b5cf-4b71-ace1-073b999c3051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Mann-Kendall Test\n",
    "mk_result_return = mk.original_test(returns)\n",
    "\n",
    "# Compute additional details\n",
    "sample_size = len(returns)\n",
    "test_statistic = mk_result_return.Tau  # Kendall's Tau correlation coefficient\n",
    "effect_size = abs(mk_result_return.Tau)  # Effect size in Kendall's Tau (absolute value)\n",
    "confidence_interval = (mk_result_return.slope - 1.96 * mk_result_return.intercept, \n",
    "                       mk_result_return.slope + 1.96 * mk_result_return.intercept)  # Approximate 95% CI\n",
    "dof = sample_size - 1  # Degrees of Freedom (approximation)\n",
    "\n",
    "# Print results\n",
    "print(\"Mann-Kendall Test for Return Period:\")\n",
    "print(f\"Trend: {mk_result_return.trend}\")\n",
    "print(f\"P-value: {mk_result_return.p}\")\n",
    "print(f\"Significance: {'Significant' if mk_result_return.p < 0.05 else 'Not Significant'}\")\n",
    "print(f\"Sen's Slope for Return Period: {mk_result_return.slope}\")\n",
    "print(f\"Sample Size: {sample_size}\")\n",
    "print(f\"Test Statistic (Tau): {test_statistic}\")\n",
    "print(f\"Effect Size: {effect_size}\")\n",
    "print(f\"Confidence Interval (Approximate 95% CI): {confidence_interval}\")\n",
    "print(f\"Degrees of Freedom (DOF): {dof}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e139f24-8846-43ef-811e-05581006f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Results\n",
    "trend_results = {\n",
    "    \"median_date_trend\": mk_result_dates.trend,\n",
    "    \"median_date_p-value\": mk_result_dates.p,\n",
    "    \"median_date_slope\": mk_result_dates.slope,\n",
    "    \"median_date_significant\": mk_result_dates.p < 0.05,\n",
    "    \"cluster_size_trend\": mk_result_sizes.trend,\n",
    "    \"cluster_size_p-value\": mk_result_sizes.p,\n",
    "    \"cluster_size_slope\": mk_result_sizes.slope,\n",
    "    \"cluster_size_significant\": mk_result_sizes.p < 0.05\n",
    "}\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame([trend_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f9e636-901a-4f98-9777-081737e7ce57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
