{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb02bd90-1681-4415-904f-f2c2bd08dd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: st_dbscan in /srv/conda/envs/notebook/lib/python3.12/site-packages (0.2.3)\n",
      "Requirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.12/site-packages (from st_dbscan) (2.2.6)\n",
      "Requirement already satisfied: scipy in /srv/conda/envs/notebook/lib/python3.12/site-packages (from st_dbscan) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /srv/conda/envs/notebook/lib/python3.12/site-packages (from st_dbscan) (1.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from scikit-learn->st_dbscan) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from scikit-learn->st_dbscan) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/st_dbscan/__init__.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install st_dbscan\n",
    "from st_dbscan import ST_DBSCAN\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.colors as mcolors\n",
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "from ST_Cluster import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59e5f1-9a33-47e4-9301-385ff4ad9c59",
   "metadata": {},
   "source": [
    "# Clustering Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe65b06e-76c1-4e03-874f-d3399b44b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_thres_list = [0.5, 1, 2, 3, 4]\n",
    "time_thres_list = [1, 3, 5, 7, 14, 30]\n",
    "num_thres_list = [3, 5, 7, 10]\n",
    "\n",
    "# Create a folder for temporary files\n",
    "temp_folder = \"Clusters\"\n",
    "os.makedirs(temp_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd0993-6baa-4629-8e1d-b39288b5d096",
   "metadata": {},
   "source": [
    "# Disaster Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b6b73e6-617e-4cf1-afff-cefd6dce998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "disasters = pd.read_csv(\"../../final_filtered_disasters.csv\")\n",
    "\n",
    "# Ensure that 'fullFIPS' is properly formatted as a 5-character string\n",
    "disasters['fullFIPS'] = disasters['fullFIPS'].astype(int).astype(str)\n",
    "disasters['fullFIPS'] = disasters['fullFIPS'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "# Preprocess 'incidentBeginDate' to numeric days\n",
    "disasters['incidentBeginDate'] = pd.to_datetime(disasters['incidentBeginDate'])  # Ensure column is datetime type\n",
    "\n",
    "origin_date = disasters['incidentBeginDate'].min()\n",
    "disasters['daysSinceStart'] = (disasters['incidentBeginDate'] - origin_date).dt.days\n",
    "disasters['date'] = pd.to_datetime(disasters['daysSinceStart'], unit='D', origin=origin_date)\n",
    "\n",
    "# Create a unique index for each row\n",
    "disasters['index'] = disasters.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65df4166-5f4c-410a-8bbc-ecfecb2191ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6388/1702220003.py:12: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['centroid_lat'] = gdf.geometry.centroid.y.round(2)\n",
      "/tmp/ipykernel_6388/1702220003.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['centroid_lon'] = gdf.geometry.centroid.x.round(2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid data saved to county_centroids.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6388/1702220003.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  centroid_df['GEOID'] = centroid_df['GEOID'].astype(int).astype(str)\n",
      "/tmp/ipykernel_6388/1702220003.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  centroid_df['GEOID'] = centroid_df['GEOID'].apply(lambda x: str(x).zfill(5))\n"
     ]
    }
   ],
   "source": [
    "# Path to your shapefile\n",
    "county_shapefile_path = '../../../Local_Data/Geospatial/tl_2019_us_county.shp'\n",
    "\n",
    "# Load the shapefile using GeoPandas\n",
    "gdf = gpd.read_file(county_shapefile_path)\n",
    "\n",
    "# Ensure the GEOID column exists (modify if your column has a different name)\n",
    "if 'GEOID' not in gdf.columns:\n",
    "    raise ValueError(\"The GEOID column is not found in the shapefile.\")\n",
    "\n",
    "# Calculate centroids and extract coordinates to two decimal places\n",
    "gdf['centroid_lat'] = gdf.geometry.centroid.y.round(2)\n",
    "gdf['centroid_lon'] = gdf.geometry.centroid.x.round(2)\n",
    "\n",
    "# Create a DataFrame with GEOID, latitude, and longitude\n",
    "centroid_df = gdf[['GEOID', 'centroid_lat', 'centroid_lon']]\n",
    "\n",
    "# Ensure that 'fullFIPS' is properly formatted as a 5-character string\n",
    "centroid_df['GEOID'] = centroid_df['GEOID'].astype(int).astype(str)\n",
    "centroid_df['GEOID'] = centroid_df['GEOID'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "# Save to CSV\n",
    "output_csv_path = 'county_centroids.csv'\n",
    "centroid_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Centroid data saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72feaa76-5d9e-4558-9848-bcf9bf4ebdf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: st_cluster_0.5_1_3, Total clusters extracted: 1196, Percent clustered: 91.30809962737791%\n",
      "Processed: st_cluster_0.5_1_5, Total clusters extracted: 824, Percent clustered: 81.27083741910178%\n",
      "Processed: st_cluster_0.5_1_7, Total clusters extracted: 560, Percent clustered: 68.75073543832124%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Drop the GEOID column if you don't need it\u001b[39;00m\n\u001b[1;32m     10\u001b[0m disasters \u001b[38;5;241m=\u001b[39m disasters\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGEOID\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m \u001b[43mdisaster_sensitivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisasters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace_thres_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_thres_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_thres_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Save only the index and cluster columns\u001b[39;00m\n\u001b[1;32m     14\u001b[0m disasters\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/clustered_disasters_sensitivity.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/2_Low_Return_Period/Production_Code/notebooks/../src/ST_Cluster.py:45\u001b[0m, in \u001b[0;36mdisaster_sensitivity\u001b[0;34m(disasters, space_thres_list, time_thres_list, num_thres_list)\u001b[0m\n\u001b[1;32m     42\u001b[0m st_dbscan \u001b[38;5;241m=\u001b[39m ST_DBSCAN(eps1\u001b[38;5;241m=\u001b[39mspace_thres, eps2\u001b[38;5;241m=\u001b[39mtime_thres, min_samples\u001b[38;5;241m=\u001b[39mnum_thres)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Fit the data\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mst_dbscan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mst_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Create a new column name for the cluster labels\u001b[39;00m\n\u001b[1;32m     48\u001b[0m cluster_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mst_cluster_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspace_thres\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_thres\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_thres\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/st_dbscan/st_dbscan.py:127\u001b[0m, in \u001b[0;36mST_DBSCAN.fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    124\u001b[0m nn_time \u001b[38;5;241m=\u001b[39m NearestNeighbors(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric,\n\u001b[1;32m    125\u001b[0m                            radius\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps2)\n\u001b[1;32m    126\u001b[0m nn_time\u001b[38;5;241m.\u001b[39mfit(X[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(n, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 127\u001b[0m time_sp \u001b[38;5;241m=\u001b[39m \u001b[43mnn_time\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradius_neighbors_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdistance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# combine both sparse matrixes and filter by time distance matrix\u001b[39;00m\n\u001b[1;32m    131\u001b[0m row \u001b[38;5;241m=\u001b[39m time_sp\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/sklearn/neighbors/_base.py:1399\u001b[0m, in \u001b[0;36mRadiusNeighborsMixin.radius_neighbors_graph\u001b[0;34m(self, X, radius, mode, sort_results)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     A_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(A_ind))\n\u001b[1;32m   1397\u001b[0m A_indptr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m), np\u001b[38;5;241m.\u001b[39mcumsum(n_neighbors)))\n\u001b[0;32m-> 1399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcsr_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_indptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples_fit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/scipy/sparse/_compressed.py:76\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy, maxprint)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n\u001b[1;32m     75\u001b[0m     copy \u001b[38;5;241m=\u001b[39m copy_if_needed\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindptr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(indptr, copy\u001b[38;5;241m=\u001b[39mcopy, dtype\u001b[38;5;241m=\u001b[39midx_dtype)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data, copy\u001b[38;5;241m=\u001b[39mcopy, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Merge the latitude and longitude columns onto the disasters dataframe\n",
    "disasters = disasters.merge(\n",
    "    centroid_df[['GEOID', 'centroid_lat', 'centroid_lon']],\n",
    "    left_on='fullFIPS',\n",
    "    right_on='GEOID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the GEOID column if you don't need it\n",
    "disasters = disasters.drop(columns=['GEOID'])\n",
    "\n",
    "disaster_sensitivity(disasters, space_thres_list, time_thres_list, num_thres_list)\n",
    "# Save only the index and cluster columns\n",
    "disasters.to_csv(f\"{temp_folder}/clustered_disasters_sensitivity.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd7f48-cd1b-49d3-9003-aa250751fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result\n",
    "disasters.to_csv(f\"{temp_folder}/clustered_disasters.csv\", index=False)\n",
    "\n",
    "print(\"Clustering complete. Output saved to 'clustered_disasters.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
