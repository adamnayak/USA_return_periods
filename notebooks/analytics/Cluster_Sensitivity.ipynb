{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551fc59-fa8d-46bb-b28b-dfea6e2eb8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "from ST_Cluster import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90098671-d71b-40be-8f3e-9f1ef7ed8123",
   "metadata": {},
   "source": [
    "# Load Disaster Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2288658c-b105-4586-a6a1-a1259963bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_df = pd.read_csv(\"../../Clusters/cluster_sensitivities_di.csv\")\n",
    "parameters = ['space_thres', 'time_thres', 'num_thres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563faf40-02d8-4df9-9a12-7f2aecacb8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ['count_complex_cases', 'percent_clustered', 'total_clusters']\n",
    "plot_scatter_lines(sensitivity_df, parameters, results)\n",
    "plot_heatmaps(sensitivity_df, 'space_thres', 'time_thres', results, figsize=(12, 8))\n",
    "plot_heatmaps(sensitivity_df, 'num_thres', 'space_thres', results, figsize=(12, 8))\n",
    "plot_heatmaps(sensitivity_df, 'time_thres', 'num_thres', results, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88f465-dd77-4a25-aafc-553974611f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ['avg_duration', 'max_duration', 'avg_lat_span', 'max_lat_span', 'avg_lon_span', 'max_lon_span']\n",
    "plot_scatter_lines(sensitivity_df, parameters, results)\n",
    "plot_heatmaps(sensitivity_df, 'space_thres', 'time_thres', results, figsize=(12, 8))\n",
    "plot_heatmaps(sensitivity_df, 'num_thres', 'space_thres', results, figsize=(12, 8))\n",
    "plot_heatmaps(sensitivity_df, 'time_thres', 'num_thres', results, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021881c9-677d-4245-99c5-7fa97bb41d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant columns\n",
    "objectives = sensitivity_df[[\"percent_clustered\", \"count_complex_cases\"]]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize 'percent_clustered' and 'total_clusters' (maximize), and 'count_complex_cases' (minimize)\n",
    "objectives[\"percent_clustered\"] = scaler.fit_transform(objectives[[\"percent_clustered\"]])\n",
    "objectives[\"count_complex_cases\"] = 1 - scaler.fit_transform(objectives[[\"count_complex_cases\"]])\n",
    "\n",
    "# Calculate a combined score (weighted equally)\n",
    "sensitivity_df[\"combined_score\"] = (\n",
    "    objectives[\"percent_clustered\"] +\n",
    "    objectives[\"count_complex_cases\"]\n",
    ")\n",
    "\n",
    "# Find the row with the highest combined score\n",
    "best_combination = sensitivity_df.loc[sensitivity_df[\"combined_score\"].idxmax()]\n",
    "\n",
    "# Display the best parameter combination and scores\n",
    "best_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49299813-ee1f-4741-a5b9-e3c03caf6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant columns\n",
    "objectives = sensitivity_df[[\"percent_clustered\", \"count_complex_cases\", \"total_clusters\"]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize metrics\n",
    "sensitivity_df[\"percent_clustered_norm\"] = scaler.fit_transform(objectives[[\"percent_clustered\"]])\n",
    "sensitivity_df[\"count_complex_cases_norm\"] = 1 - scaler.fit_transform(objectives[[\"count_complex_cases\"]])\n",
    "sensitivity_df[\"total_clusters_norm\"] = scaler.fit_transform(objectives[[\"total_clusters\"]])\n",
    "\n",
    "# Define weights for each metric\n",
    "weights = {\n",
    "    \"percent_clustered_norm\": 1.0,  # Weight for percent_clustered (maximize)\n",
    "    \"count_complex_cases_norm\": 1.0,  # Weight for count_complex_cases (minimize)\n",
    "    \"total_clusters_norm\": 1.0,  # Weight for total_clusters (maximize)\n",
    "}\n",
    "\n",
    "# Calculate a combined score with weights\n",
    "sensitivity_df[\"combined_score\"] = (\n",
    "    sensitivity_df[\"percent_clustered_norm\"] * weights[\"percent_clustered_norm\"] +\n",
    "    sensitivity_df[\"count_complex_cases_norm\"] * weights[\"count_complex_cases_norm\"] +\n",
    "    sensitivity_df[\"total_clusters_norm\"] * weights[\"total_clusters_norm\"]\n",
    ")\n",
    "\n",
    "# Find the row with the highest combined score\n",
    "best_combination = sensitivity_df.loc[sensitivity_df[\"combined_score\"].idxmax()]\n",
    "\n",
    "# Display the best parameter combination and scores\n",
    "print(best_combination)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497308c0-a19e-4dd0-9153-a3fbe015e8ab",
   "metadata": {},
   "source": [
    "# Load Claims Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593544b6-5664-40b0-97e9-03cb35777e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_df = pd.read_csv(\"../../Clusters/cluster_sensitivities_cl.csv\")\n",
    "parameters = ['space_thres', 'time_thres', 'num_thres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48bac48-5066-452c-af89-259757c999e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ['unclustered_percentage', 'total_clusters']\n",
    "plot_scatter_lines(sensitivity_df, parameters, results)\n",
    "plot_heatmaps(sensitivity_df, 'space_thres', 'time_thres', results, figsize=(12, 8))\n",
    "plot_heatmaps(sensitivity_df, 'num_thres', 'space_thres', results, figsize=(12, 8))\n",
    "plot_heatmaps(sensitivity_df, 'time_thres', 'num_thres', results, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee079c-c990-4264-b304-c5ffed04ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ['avg_duration', 'max_duration', 'avg_lat_span', 'max_lat_span', 'avg_lon_span', 'max_lon_span']\n",
    "plot_scatter_lines(sensitivity_df, parameters, results)\n",
    "plot_heatmaps(sensitivity_df, 'space_thres', 'time_thres', results, figsize=(12, 8))\n",
    "plot_heatmaps(sensitivity_df, 'num_thres', 'space_thres', results, figsize=(12, 8))\n",
    "plot_heatmaps(sensitivity_df, 'time_thres', 'num_thres', results, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db542c-cd1a-4b2f-a9be-57d1fbf4b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ['var_duration', 'var_lat_span', 'var_lon_span']\n",
    "plot_scatter_lines(sensitivity_df, parameters, results)\n",
    "plot_heatmaps(sensitivity_df, 'space_thres', 'time_thres', results, figsize=(12, 8))\n",
    "plot_heatmaps(sensitivity_df, 'num_thres', 'space_thres', results, figsize=(12, 8))\n",
    "plot_heatmaps(sensitivity_df, 'time_thres', 'num_thres', results, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a798d-55d4-4fa7-9990-15bd862ffbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine objectives for optimization\n",
    "objectives = [\n",
    "    (\"unclustered_percentage\", \"min\"),  # Minimize\n",
    "    (\"total_clusters\", \"max\"),  # Maximize\n",
    "]\n",
    "\n",
    "# Define constraints\n",
    "max_duration_limit = 90 \n",
    "max_lon_span_limit = 120\n",
    "max_lat_span_limit = 25\n",
    "\n",
    "# Create a weighted objective score function\n",
    "def compute_objective_score(row):\n",
    "    score = 0\n",
    "    for column, goal in objectives:\n",
    "        if goal == \"max\":\n",
    "            score += row[column]\n",
    "        elif goal == \"min\":\n",
    "            score -= row[column]\n",
    "    return score\n",
    "\n",
    "# Apply constraints to filter the dataset\n",
    "constrained_data = sensitivity_df[\n",
    "    (sensitivity_df[\"max_duration\"] < max_duration_limit) &\n",
    "    (sensitivity_df[\"max_lon_span\"] < max_lon_span_limit) &\n",
    "    (sensitivity_df[\"max_lat_span\"] < max_lat_span_limit)\n",
    "]\n",
    "\n",
    "# Recompute the optimal parameters under constraints\n",
    "if not constrained_data.empty:\n",
    "    constrained_data[\"objective_score\"] = constrained_data.apply(compute_objective_score, axis=1)\n",
    "    optimal_parameters_constrained = constrained_data.loc[\n",
    "        constrained_data[\"objective_score\"].idxmax(), [\"space_thres\", \"time_thres\", \"num_thres\"]\n",
    "    ]\n",
    "else:\n",
    "    optimal_parameters_constrained = \"No feasible solution under given constraints\"\n",
    "\n",
    "optimal_parameters_constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c9e3f5-4521-45cb-b0f1-25bf3158dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constraints\n",
    "max_duration_limit = 90\n",
    "max_lon_span_limit = 120\n",
    "max_lat_span_limit = 25\n",
    "\n",
    "# Apply constraints to filter the dataset\n",
    "filtered_df = sensitivity_df[\n",
    "    (sensitivity_df[\"max_duration\"] < max_duration_limit) &\n",
    "    (sensitivity_df[\"max_lon_span\"] < max_lon_span_limit) &\n",
    "    (sensitivity_df[\"max_lat_span\"] < max_lat_span_limit)\n",
    "]\n",
    "\n",
    "# Normalize the objectives\n",
    "if not filtered_df.empty:\n",
    "    objectives = filtered_df[\n",
    "        [\"unclustered_percentage\", \"total_clusters\"]\n",
    "    ]\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Maximize certain metrics and minimize 'count_complex_cases'\n",
    "    objectives[\"unclustered_percentage\"] = 1 - scaler.fit_transform(objectives[[\"unclustered_percentage\"]])\n",
    "    objectives[\"total_clusters\"] = scaler.fit_transform(objectives[[\"total_clusters\"]])\n",
    "\n",
    "    # Calculate a combined score (weighted equally)\n",
    "    filtered_df[\"combined_score\"] = objectives.sum(axis=1)\n",
    "\n",
    "    # Find the row with the highest combined score\n",
    "    best_combination = filtered_df.loc[filtered_df[\"combined_score\"].idxmax()]\n",
    "\n",
    "    # Display the best parameter combination and scores\n",
    "    print(\"Best Parameter Combination and Scores:\")\n",
    "    print(best_combination)\n",
    "else:\n",
    "    print(\"No feasible solution under the given constraints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef6463-cdfe-4563-bbe1-9a5a296541ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constraints\n",
    "max_duration_limit = 90\n",
    "max_lon_span_limit = 120\n",
    "max_lat_span_limit = 25\n",
    "\n",
    "# Apply constraints to filter the dataset\n",
    "filtered_df = sensitivity_df[\n",
    "    (sensitivity_df[\"max_duration\"] < max_duration_limit) &\n",
    "    (sensitivity_df[\"max_lon_span\"] < max_lon_span_limit) &\n",
    "    (sensitivity_df[\"max_lat_span\"] < max_lat_span_limit)\n",
    "]\n",
    "\n",
    "# Normalize the objectives\n",
    "if not filtered_df.empty:\n",
    "    # Only consider 'unclustered_percentage' for objective calculation\n",
    "    objectives = filtered_df[[\"unclustered_percentage\"]]\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Maximize 'unclustered_percentage' (by minimizing the complement)\n",
    "    objectives[\"unclustered_percentage\"] = 1 - scaler.fit_transform(objectives[[\"unclustered_percentage\"]])\n",
    "\n",
    "    # Add normalized objective score to the filtered dataframe\n",
    "    filtered_df[\"combined_score\"] = objectives.sum(axis=1)\n",
    "\n",
    "    # Find the row with the highest combined score\n",
    "    best_combination = filtered_df.loc[filtered_df[\"combined_score\"].idxmax()]\n",
    "\n",
    "    # Display the best parameter combination and scores\n",
    "    print(\"Best Parameter Combination and Scores:\")\n",
    "    print(best_combination)\n",
    "else:\n",
    "    print(\"No feasible solution under the given constraints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fd9d1-f2e8-444c-b4fb-43359cf94808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
