{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be0e3f-6073-455b-9ba4-f2dbce34221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "import os\n",
    "import shutil\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from rasterio.mask import mask\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "from PRISM_Preprocess import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f807608-c7de-4c3b-93ef-84dc13c3f4a6",
   "metadata": {},
   "source": [
    "# Extract PRISM Data by County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9edbe53-88ca-435b-a99d-ff66f09f7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GCS client\n",
    "client = storage.Client()\n",
    "gcs = gcsfs.GCSFileSystem()\n",
    "\n",
    "# Define GCS bucket and folder paths\n",
    "bucket_name = \"leap-persistent\"\n",
    "source_folder = \"adamnayak/flood-insurance/PRISM/Monthly_Annual\"\n",
    "destination_folder = \"adamnayak/flood-insurance/PRISM/County_Monthly_Precip_By_Year_Mon\"\n",
    "\n",
    "# Define local folders\n",
    "local_prism_folder = \"PRISM_Data\"\n",
    "output_folder = \"PRISM_Monthly_Precip\"\n",
    "county_shapefile = \"../Local_Data/Geospatial/tl_2019_us_county.shp\"\n",
    "\n",
    "# Ensure local folders exist\n",
    "os.makedirs(local_prism_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc26d2e-ce82-4a47-a84f-baa707d71b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Download PRISM files from GCS to local `prism_folder`\n",
    "print(\"Downloading PRISM files...\")\n",
    "bucket = client.bucket(bucket_name)\n",
    "blobs = client.list_blobs(bucket_name, prefix=source_folder)\n",
    "\n",
    "for blob in blobs:\n",
    "    if not blob.name.endswith(\"/\"):  # Skip directories\n",
    "        destination_path = os.path.join(local_prism_folder, os.path.basename(blob.name))\n",
    "        blob.download_to_filename(destination_path)\n",
    "        print(f\"Downloaded: {blob.name} to {destination_path}\")\n",
    "\n",
    "# Load the county shapefile\n",
    "counties = gpd.read_file(county_shapefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3432f94-f3ca-4cc0-a690-6b0b2a2e1753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Process each PRISM file\n",
    "for year in range(1923, 2024):\n",
    "    for month in range(1, 13):\n",
    "        yearmo = f\"{year}{str(month).zfill(2)}\"\n",
    "        \n",
    "        # Determine file naming pattern based on year\n",
    "        file_suffix = \"M2\" if year < 1980 else \"M3\"\n",
    "        \n",
    "        prism_file = os.path.join(local_prism_folder, f\"PRISM_ppt_stable_4km{file_suffix}_{yearmo}_bil.bil\")\n",
    "        \n",
    "        # Skip if the file does not exist\n",
    "        if not os.path.exists(prism_file):\n",
    "            print(f\"File not found: {prism_file}\")\n",
    "            continue\n",
    "        \n",
    "        with rasterio.open(prism_file) as src:\n",
    "            # Ensure that the precipitation data has a CRS\n",
    "            if not ds.rio.crs:\n",
    "                # If the file lacks metadata, assume WGS84\n",
    "                ds = ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "                print(\"Assigned EPSG:4326 to precipitation data:\", ds.rio.crs)\n",
    "\n",
    "            county_precip = []\n",
    "            \n",
    "            for _, county in counties.iterrows():\n",
    "                geom = [county[\"geometry\"].__geo_interface__]\n",
    "                \n",
    "                # Buffer the geometry slightly to deal with precision issues\n",
    "                buffered_geom = [county[\"geometry\"].buffer(0.001).__geo_interface__]\n",
    "                \n",
    "                avg_precip = calculate_weighted_average(src, buffered_geom)\n",
    "                \n",
    "                county_precip.append({\n",
    "                    \"county\": county[\"GEOID\"],\n",
    "                    \"precipitation\": avg_precip\n",
    "                })\n",
    "        \n",
    "        # Create a DataFrame and save to CSV\n",
    "        df = pd.DataFrame(county_precip)\n",
    "        output_file = os.path.join(output_folder, f\"{yearmo}_PRISM_monthly_precip.csv\")\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e478cbe-eec4-4b6f-9c27-ba21b0a43e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Concatenate and save all files\n",
    "# Define the folder containing the CSV files\n",
    "folder_path = 'PRISM_Monthly_Precip'\n",
    "\n",
    "# Initialize an empty list to store the DataFrames\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):  # Ensure we only read CSV files\n",
    "        # Extract the year and month from the filename\n",
    "        yearmo = file_name.split('_')[0]  # Format: {yearmo}_PRISM_monthly_precip.csv\n",
    "        year = yearmo[:4]  # First 4 characters are the year\n",
    "        month = yearmo[4:6]  # Next 2 characters are the month\n",
    "        \n",
    "        # Load the CSV into a DataFrame\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add 'year' and 'month' columns\n",
    "        df['year'] = year\n",
    "        df['month'] = month\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        all_data.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames in the list\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "final_df.rename(columns={'precipitation': 'PRISM_precipitation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3eca0-d9a6-4390-a0b9-c4254511c6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Find the annual maximum precipitation for each county\n",
    "annual_max_df = final_df.groupby(['county', 'year'])['PRISM_precipitation'].max().reset_index()\n",
    "annual_max_df = annual_max_df.rename(columns={'PRISM_precipitation': 'PRISM_annual_max_precipitation'})\n",
    "\n",
    "# Step 5: Merge the annual maximum values back into the original PRISM_df\n",
    "final_df = pd.merge(final_df, annual_max_df, on=['county', 'year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5479c4e4-f578-4a1c-b5e6-c8c257ff9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create a dictionary that stores annual max distributions for each county\n",
    "county_max_dist = final_df.groupby('county')['PRISM_annual_max_precipitation'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07278bf8-a8cc-4ccc-809e-4b047bff8de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 7: Apply the vectorized function to calculate percentiles for the entire DataFrame\n",
    "start_time = time.time()\n",
    "row_count = len(final_df)\n",
    "batch_size = 10000\n",
    "\n",
    "# List to hold the results\n",
    "percentile_results = []\n",
    "\n",
    "for i in range(0, row_count, batch_size):\n",
    "    batch_df = final_df.iloc[i:i + batch_size]  # Process rows in batches of 10,000\n",
    "    # Calculate the percentiles for the batch\n",
    "    batch_percentiles = batch_df.apply(\n",
    "        lambda row: calculate_percentile_vectorized(row['PRISM_precipitation'], row['county'], county_max_dist),\n",
    "        axis=1\n",
    "    )\n",
    "    # Append results to the list\n",
    "    percentile_results.extend(batch_percentiles)\n",
    "    \n",
    "    # Print progress every 10,000 rows\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Processed {i + batch_size} rows out of {row_count} (Elapsed time: {elapsed_time:.2f} seconds)\")\n",
    "\n",
    "# Step 8: Assign the results back to the DataFrame\n",
    "final_df['PRISM_percentile'] = percentile_results\n",
    "final_df['PRISM_percentile'] = final_df['PRISM_percentile']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf468b5d-25da-4d58-bd53-5793791f57a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the final DataFrame to a CSV file\n",
    "output_file_path = 'PRISM_Monthly_Precip_Processed_County.csv'\n",
    "final_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Data has been successfully concatenated and saved to {output_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d8352-8f5f-4cbc-b5d5-75005069df26",
   "metadata": {},
   "source": [
    "# Merge with Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fed55a-2b68-48cd-bdd2-390b495e4e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'year' and 'month' columns in PRISM_df are strings as well\n",
    "final_df['year'] = final_df['year'].astype(str)\n",
    "final_df['month'] = final_df['month'].astype(str).str.zfill(2)\n",
    "final_df['county'] = final_df['county'].astype(str).str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c3736-81ba-4513-870a-5df684a84800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the claims data and drop NAs\n",
    "claims_df = pd.read_csv('MSWEP_ERA5_Processed_Claims.csv')\n",
    "claims_df = claims_df.dropna(subset=['countyCode', 'dateOfLoss'])\n",
    "\n",
    "# Ensure dateOfLoss is in datetime format\n",
    "claims_df['dateOfLoss'] = pd.to_datetime(claims_df['dateOfLoss'], errors='coerce')\n",
    "\n",
    "# Extract year and month from 'dateOfLoss'\n",
    "claims_df['year'] = claims_df['dateOfLoss'].dt.year.astype(str)  # Convert to string\n",
    "claims_df['month'] = claims_df['dateOfLoss'].dt.month.astype(str).str.zfill(2)  # Convert to string and pad month with leading zeros\n",
    "\n",
    "# Convert 'countyCode' in the claims data to string and ensure it has leading zeros to match PRISM_df\n",
    "claims_df['countyCode'] = claims_df['countyCode'].astype(int).astype(str)\n",
    "claims_df['countyCode'] = claims_df['countyCode'].apply(lambda x: str(x).zfill(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939ac5ca-b020-448e-af80-b885cfae8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the claims data with PRISM_df based on 'year', 'month', and 'countyCode' from claims, and 'county' from PRISM_df\n",
    "merged_df = pd.merge(\n",
    "    claims_df,\n",
    "    final_df[['year', 'month', 'county', 'PRISM_precipitation', 'PRISM_percentile']],\n",
    "    how='left',\n",
    "    left_on=['year', 'month', 'countyCode'],\n",
    "    right_on=['year', 'month', 'county']\n",
    ").drop(columns=['year', 'month', 'county'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5124617-d88a-49cf-b3ff-31edf5e6ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.rename(columns={'PRISM_precipitation': 'PRISM_mon_precipitation',\n",
    "                         'PRISM_percentile': 'PRISM_mon_percentile'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5be37-a0a5-4262-969b-12bfc5d437ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('PRISM_MSWEP_ERA5_Processed_Claims.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66eb11-5675-4138-9f64-0c0917c0a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the local `prism_folder` after processing\n",
    "print(f\"Deleting local folder: {local_prism_folder}\")\n",
    "shutil.rmtree(local_prism_folder)\n",
    "\n",
    "# Upload processed data to GCS\n",
    "print(\"Uploading processed data to GCS...\")\n",
    "output_blobs = os.listdir(output_folder)\n",
    "for file_name in output_blobs:\n",
    "    local_path = os.path.join(output_folder, file_name)\n",
    "    blob_name = os.path.join(destination_folder, file_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(local_path)\n",
    "    print(f\"Uploaded: {file_name} to {blob_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
