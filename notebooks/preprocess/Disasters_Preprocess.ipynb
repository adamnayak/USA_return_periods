{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda65d8-8cf1-4f36-8a04-8312c05d8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "import time  # To track the elapsed time\n",
    "import matplotlib.colors as mcolors\n",
    "import concurrent.futures\n",
    "import re\n",
    "!pip install fuzzywuzzy\n",
    "from fuzzywuzzy import process \n",
    "import matplotlib.patches as mpatches\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd027711-b074-4e9e-8a4b-2fcecca5a454",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb443c-e0f6-4fb0-a520-0b33806361fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to shift the precipitation data columns based on the incidentBeginDate rather than incidentEndDate\n",
    "def shift_precipitation_data(df, column, shift_days):\n",
    "    \"\"\" Shift the given column by shift_days and return the modified dataframe. \"\"\"\n",
    "    df[column] = df[column].shift(-shift_days)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5cf18a-52db-4704-82c6-67c76cb0121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge precipitation data for a given county FIPS\n",
    "def merge_precip_data(filtered_disasters_county, county_fips):\n",
    "    # Convert 'incidentBeginDate' to a date format (without time) for matching\n",
    "    filtered_disasters_county['incidentBeginDate'] = pd.to_datetime(filtered_disasters_county['incidentBeginDate']).dt.date\n",
    "    \n",
    "    # Load MSWEP precipitation data for this county\n",
    "    mswep_file = os.path.join(mswep_dir, f'{county_fips}_precip_processed.csv')\n",
    "    if os.path.exists(mswep_file):\n",
    "        mswep_data = pd.read_csv(mswep_file)\n",
    "        mswep_data['county'] = mswep_data['county'].astype(str).str.zfill(5)  # Ensure county FIPS is a 5-character string\n",
    "        \n",
    "        # Convert 'date' to datetime format and extract only the date (without time)\n",
    "        mswep_data['date'] = pd.to_datetime(mswep_data['date']).dt.date\n",
    "        \n",
    "        # Shift necessary MSWEP columns\n",
    "        mswep_data = shift_precipitation_data(mswep_data, 'MSWEP_precipitation_30d_sum', 29)\n",
    "        mswep_data = shift_precipitation_data(mswep_data, 'MSWEP_precipitation_30d_sum_percentile_modeled', 29)\n",
    "        mswep_data = shift_precipitation_data(mswep_data, 'MSWEP_precipitation_30d_max_1d', 29)\n",
    "        mswep_data = shift_precipitation_data(mswep_data, 'MSWEP_precipitation_30d_max_1d_percentile_modeled', 29)\n",
    "        mswep_data = shift_precipitation_data(mswep_data, 'MSWEP_precipitation_30d_max_3d', 29)\n",
    "        mswep_data = shift_precipitation_data(mswep_data, 'MSWEP_precipitation_30d_max_3d_percentile_modeled', 29)\n",
    "        mswep_data = shift_precipitation_data(mswep_data, 'MSWEP_precipitation_30d_max_5d', 29)\n",
    "        mswep_data = shift_precipitation_data(mswep_data, 'MSWEP_precipitation_30d_max_5d_percentile_modeled', 29)\n",
    "        mswep_data = shift_precipitation_data(mswep_data, 'MSWEP_precipitation_30d_max_7d', 29)\n",
    "        mswep_data = shift_precipitation_data(mswep_data, 'MSWEP_precipitation_30d_max_7d_percentile_modeled', 29)\n",
    "        mswep_data = shift_precipitation_data(mswep_data, 'MSWEP_precipitation_30d_max_14d', 29)\n",
    "        mswep_data = shift_precipitation_data(mswep_data, 'MSWEP_precipitation_30d_max_14d_percentile_modeled', 29)\n",
    "\n",
    "        # Merge MSWEP data with disaster data based on 'incidentBeginDate'\n",
    "        filtered_disasters_county = pd.merge(filtered_disasters_county, mswep_data, left_on=['incidentBeginDate', 'fullFIPS'], right_on=['date', 'county'], how='left')\n",
    "\n",
    "        # Drop unwanted columns from the merged DataFrame\n",
    "        filtered_disasters_county = filtered_disasters_county.drop(columns=['county', 'date', 'year'], errors='ignore')\n",
    "      \n",
    "    # Load ERA5 precipitation data for this county\n",
    "    era5_file = os.path.join(era5_dir, f'{county_fips}_precip_processed.csv')\n",
    "    if os.path.exists(era5_file):\n",
    "        era5_data = pd.read_csv(era5_file)\n",
    "        era5_data['county'] = era5_data['county'].astype(str).str.zfill(5)  # Ensure county FIPS is a 5-character string\n",
    "        \n",
    "        # Convert 'date' to datetime format and extract only the date\n",
    "        era5_data['date'] = pd.to_datetime(era5_data['date']).dt.date\n",
    "        \n",
    "        # Shift necessary ERA5 columns\n",
    "        era5_data = shift_precipitation_data(era5_data, 'ERA5_precipitation_30d_sum', 29)\n",
    "        era5_data = shift_precipitation_data(era5_data, 'ERA5_precipitation_30d_sum_percentile_modeled', 29)\n",
    "        era5_data = shift_precipitation_data(era5_data, 'ERA5_precipitation_30d_max_1d', 29)\n",
    "        era5_data = shift_precipitation_data(era5_data, 'ERA5_precipitation_30d_max_1d_percentile_modeled', 29)\n",
    "        era5_data = shift_precipitation_data(era5_data, 'ERA5_precipitation_30d_max_3d', 29)\n",
    "        era5_data = shift_precipitation_data(era5_data, 'ERA5_precipitation_30d_max_3d_percentile_modeled', 29)\n",
    "        era5_data = shift_precipitation_data(era5_data, 'ERA5_precipitation_30d_max_5d', 29)\n",
    "        era5_data = shift_precipitation_data(era5_data, 'ERA5_precipitation_30d_max_5d_percentile_modeled', 29)\n",
    "        era5_data = shift_precipitation_data(era5_data, 'ERA5_precipitation_30d_max_7d', 29)\n",
    "        era5_data = shift_precipitation_data(era5_data, 'ERA5_precipitation_30d_max_7d_percentile_modeled', 29)\n",
    "        era5_data = shift_precipitation_data(era5_data, 'ERA5_precipitation_30d_max_14d', 29)\n",
    "        era5_data = shift_precipitation_data(era5_data, 'ERA5_precipitation_30d_max_14d_percentile_modeled', 29)\n",
    "\n",
    "        # Merge ERA5 data with disaster data based on 'incidentBeginDate'\n",
    "        filtered_disasters_county = pd.merge(filtered_disasters_county, era5_data, left_on=['incidentBeginDate', 'fullFIPS'], right_on=['date', 'county'], how='left')\n",
    "\n",
    "        # Drop unwanted columns from the merged DataFrame\n",
    "        filtered_disasters_county = filtered_disasters_county.drop(columns=['county', 'date', 'year'], errors='ignore')\n",
    "    \n",
    "    return filtered_disasters_county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76972480-fa93-4891-a9f2-bc5744c3ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single county\n",
    "def process_county(county_fips, county_group, start_time):\n",
    "    # Ensure fullFIPS is a string with leading zeros\n",
    "    county_group['fullFIPS'] = county_group['fullFIPS'].astype(str).str.zfill(5)\n",
    "    \n",
    "    # Merge precipitation data for this county\n",
    "    merged_county_data = merge_precip_data(county_group, county_fips)\n",
    "    \n",
    "    # Calculate elapsed time for this county\n",
    "    county_elapsed_time = time.time() - start_time\n",
    "    print(f\"Processed county {county_fips} in {county_elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return merged_county_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64b913-65cb-444c-b860-82ad09c17625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_gcs_folder(gcs_folder_path, local_destination):\n",
    "    \"\"\"\n",
    "    Downloads a folder from Google Cloud Storage to a local directory using the Google Cloud Storage client library.\n",
    "    \n",
    "    Parameters:\n",
    "    - gcs_folder_path (str): The GCS path of the folder (e.g., 'bucket-name/path/to/folder').\n",
    "    - local_destination (str): The local directory where the folder will be downloaded.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Extract bucket name and prefix from the GCS path\n",
    "    if not gcs_folder_path.startswith(\"gs://\"):\n",
    "        raise ValueError(\"The GCS folder path must start with 'gs://'\")\n",
    "    \n",
    "    gcs_folder_path = gcs_folder_path[5:]  # Remove 'gs://'\n",
    "    bucket_name, folder_prefix = gcs_folder_path.split(\"/\", 1)\n",
    "    \n",
    "    # Initialize the Google Cloud Storage client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    # List all blobs in the folder\n",
    "    blobs = bucket.list_blobs(prefix=folder_prefix)\n",
    "    \n",
    "    # Ensure the local destination exists\n",
    "    os.makedirs(local_destination, exist_ok=True)\n",
    "    \n",
    "    for blob in blobs:\n",
    "        # Define the local file path\n",
    "        relative_path = os.path.relpath(blob.name, folder_prefix)\n",
    "        local_file_path = os.path.join(local_destination, relative_path)\n",
    "        \n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "        \n",
    "        # Download the blob\n",
    "        print(f\"Downloading {blob.name} to {local_file_path}\")\n",
    "        blob.download_to_filename(local_file_path)\n",
    "    \n",
    "    print(f\"Successfully downloaded folder {gcs_folder_path} to {local_destination}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e447a7-5c9c-4809-bcfd-ff5783c6f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_gcs_file(gcs_file_path, local_destination):\n",
    "    \"\"\"\n",
    "    Downloads a single file from Google Cloud Storage to a local directory using the Google Cloud Storage client library.\n",
    "    \n",
    "    Parameters:\n",
    "    - gcs_file_path (str): The GCS path of the file (e.g., 'gs://bucket-name/path/to/file.csv').\n",
    "    - local_destination (str): The local file path where the file will be downloaded.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    \n",
    "    # Ensure the GCS file path starts with 'gs://'\n",
    "    if not gcs_file_path.startswith(\"gs://\"):\n",
    "        raise ValueError(\"The GCS file path must start with 'gs://'\")\n",
    "    \n",
    "    # Extract bucket name and file path\n",
    "    gcs_file_path = gcs_file_path[5:]  # Remove 'gs://'\n",
    "    bucket_name, file_path = gcs_file_path.split(\"/\", 1)\n",
    "    \n",
    "    # Initialize the Google Cloud Storage client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_path)\n",
    "    \n",
    "    # Ensure the local directory exists\n",
    "    os.makedirs(os.path.dirname(local_destination), exist_ok=True)\n",
    "    \n",
    "    # Download the blob to the local file\n",
    "    print(f\"Downloading {file_path} to {local_destination}\")\n",
    "    blob.download_to_filename(local_destination)\n",
    "    \n",
    "    print(f\"Successfully downloaded file {gcs_file_path} to {local_destination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba8372-538e-4927-9d44-240651dc7a36",
   "metadata": {},
   "source": [
    "# Filter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b055f-ba3a-4420-8928-efb24c2e5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull = False # if you need to pull data from cloud\n",
    "push = False # if you need to delete the data after\n",
    "precipPresent = True\n",
    "monthlyMask = True\n",
    "percentileMask = False\n",
    "\n",
    "mon_thres = 0 # Filter out below mon_thres monthly precipitation values as erroneous\n",
    "perc_thres = 0 # Filter out below perc_thres th percentile values as erroneous for weekly precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb975005-14c2-4a1a-8b26-61a774d8fb6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if pull:\n",
    "    os.makedirs('MSWEP_Daily_Precip_Processed_County', exist_ok=True)\n",
    "    os.makedirs('ERA5_Daily_Precip_Processed_County', exist_ok=True)\n",
    "    \n",
    "    download_gcs_folder('gs://leap-persistent/adamnayak/flood-insurance/MSWEP/MSWEP_Daily_Precip_Processed_County', 'MSWEP_Daily_Precip_Processed_County')\n",
    "    download_gcs_folder('gs://leap-persistent/adamnayak/flood-insurance/ERA5/ERA5_Daily_Precip_Processed_County', 'ERA5_Daily_Precip_Processed_County')\n",
    "\n",
    "    # Get the current directory\n",
    "    current_directory = os.getcwd()\n",
    "    \n",
    "    # Construct the local destination path\n",
    "    local_destination = os.path.join(current_directory, 'PRISM_Monthly_Precip_Processed_County.csv')\n",
    "    \n",
    "    # Call the function to download\n",
    "    download_gcs_file('gs://leap-persistent/adamnayak/flood-insurance/PRISM/PRISM_Monthly_Precip_Processed_County.csv', local_destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047520e-4edb-42ab-bfa6-99b24018338b",
   "metadata": {},
   "source": [
    "# Load Disasters Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150843a-a21e-4483-9db6-2ab1d444ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "disasters = pd.read_csv('../../../Local_Data/FEMA_Disaster_Asst_Data/DisasterDeclarationsSummaries.csv')\n",
    "\n",
    "# List of incident types to filter\n",
    "incident_types = [\n",
    "    'Coastal Storm',\n",
    "    'Dam/Levee Break',\n",
    "    'Flood',\n",
    "    'Hurricane',\n",
    "    'Severe Storm',\n",
    "    'Tropical Storm',\n",
    "    'Typhoon',\n",
    "] #'Winter Storm', 'Snowstorm', 'Severe Ice Storm'\n",
    "\n",
    "# Print number of unfiltered records\n",
    "print(f\"{len(disasters)} unfiltered disaster total records found\")\n",
    "\n",
    "# Filtering the dataframe\n",
    "filtered_disasters = disasters[disasters['incidentType'].isin(incident_types)]\n",
    "\n",
    "# Print number of unfiltered records\n",
    "print(f\"{len(filtered_disasters)} flood disaster total records found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5097d-c3fe-402d-aef1-15fb5ec9201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine \"Coastal Storm\" and \"Severe Storm\" into a new category \"Storm\"\n",
    "filtered_disasters['incidentType'] = filtered_disasters['incidentType'].replace({\n",
    "    'Coastal Storm': 'Storm',\n",
    "    'Severe Storm': 'Storm'\n",
    "})\n",
    "\n",
    "# Combine \"Hurricane\" and \"Tropical Storm\" into \"Hurricane/Tropical Cyclone\" in `final_disasters_df`\n",
    "filtered_disasters['incidentType'] = filtered_disasters['incidentType'].replace(\n",
    "    {'Hurricane': 'Hurricane/Tropical Cyclone', 'Tropical Storm': 'Hurricane/Tropical Cyclone'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035cb9d-0e28-472e-b2f5-24918f9cec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map of two-letter state codes to FIPS codes\n",
    "state_fips_mapping = {\n",
    "    'AL': '01', 'AK': '02', 'AZ': '04', 'AR': '05', 'CA': '06', 'CO': '08', 'CT': '09', 'DE': '10', \n",
    "    'DC': '11', 'FL': '12', 'GA': '13', 'HI': '15', 'ID': '16', 'IL': '17', 'IN': '18', 'IA': '19',\n",
    "    'KS': '20', 'KY': '21', 'LA': '22', 'ME': '23', 'MD': '24', 'MA': '25', 'MI': '26', 'MN': '27', \n",
    "    'MS': '28', 'MO': '29', 'MT': '30', 'NE': '31', 'NV': '32', 'NH': '33', 'NJ': '34', 'NM': '35',\n",
    "    'NY': '36', 'NC': '37', 'ND': '38', 'OH': '39', 'OK': '40', 'OR': '41', 'PA': '42', 'RI': '44',\n",
    "    'SC': '45', 'SD': '46', 'TN': '47', 'TX': '48', 'UT': '49', 'VT': '50', 'VA': '51', 'WA': '53', \n",
    "    'WV': '54', 'WI': '55', 'WY': '56', 'PR': '72'  # Add more as needed\n",
    "}\n",
    "\n",
    "# Map the state column to FIPS codes\n",
    "filtered_disasters['stateFIPS'] = filtered_disasters['state'].map(state_fips_mapping)\n",
    "\n",
    "# Ensure the county code is 3 characters, padded with leading zeros if necessary\n",
    "filtered_disasters['fipsCountyCode'] = filtered_disasters['fipsCountyCode'].astype(str).str.zfill(5)\n",
    "\n",
    "# Combine stateFIPS and fipsCountyCode to create the full 5-digit FIPS code\n",
    "filtered_disasters['fullFIPS'] = filtered_disasters['stateFIPS'] + filtered_disasters['fipsCountyCode']\n",
    "\n",
    "# Now filter based on the full FIPS code\n",
    "contiguous_us_states_fips = [f\"{i:02d}\" for i in range(1, 57) if i not in [2, 15, 60]]\n",
    "filtered_disasters_contiguous = filtered_disasters[filtered_disasters['fullFIPS'].str[:2].isin(contiguous_us_states_fips)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294ad88-be16-4455-b86f-9dc7fa7d9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the date fields are in datetime format\n",
    "df = filtered_disasters_contiguous.copy()\n",
    "df['declarationDate'] = pd.to_datetime(df['declarationDate'])\n",
    "df['disasterCloseoutDate'] = pd.to_datetime(df['disasterCloseoutDate'])\n",
    "df['incidentBeginDate'] = pd.to_datetime(df['incidentBeginDate'])\n",
    "df['incidentEndDate'] = pd.to_datetime(df['incidentEndDate'])\n",
    "\n",
    "# Calculate the number of days from declarationDate\n",
    "df['days_to_disasterCloseout'] = (df['disasterCloseoutDate'] - df['declarationDate']).dt.days\n",
    "df['days_to_incidentBegin'] = (df['incidentBeginDate'] - df['declarationDate']).dt.days\n",
    "df['days_to_incidentEnd'] = (df['incidentEndDate'] - df['declarationDate']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac49d0-ec22-4d9d-b5db-743ca3325a10",
   "metadata": {},
   "source": [
    "# Pull Processed Precipitation Data from the Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26779ce5-9814-4bb9-83d0-7aa9e0d8d33d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Path to MSWEP and ERA5 directories\n",
    "mswep_dir = 'MSWEP_Daily_Precip_Processed_County'\n",
    "era5_dir = 'ERA5_Daily_Precip_Processed_County'\n",
    "\n",
    "# Start time for performance tracking\n",
    "start_time = time.time()\n",
    "\n",
    "# Use ThreadPoolExecutor to process counties in parallel\n",
    "merged_disasters = []\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Create a dictionary to map each future to its county_fips and county_group\n",
    "    futures = {\n",
    "        executor.submit(process_county, county_fips, county_group, start_time): county_fips\n",
    "        for county_fips, county_group in filtered_disasters_contiguous.groupby('fullFIPS')\n",
    "    }\n",
    "    \n",
    "    # As each future completes, gather the result and print progress\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        county_fips = futures[future]\n",
    "        try:\n",
    "            merged_disasters.append(future.result())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing county {county_fips}: {e}\")\n",
    "\n",
    "# Concatenate all county dataframes back into a single dataframe\n",
    "final_disasters_df = pd.concat(merged_disasters, ignore_index=True)\n",
    "\n",
    "# Print the total elapsed time\n",
    "total_elapsed_time = time.time() - start_time\n",
    "print(f\"All counties processed in {total_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d7b14-64b4-4777-97a9-47e21fb817a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRISM_precip = pd.read_csv('../../PRISM_Monthly_Precip_Processed_County.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a6381-426f-4ea4-b2da-85c090213ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract year and month from incidentBeginDate\n",
    "final_disasters_df['year'] = pd.to_datetime(final_disasters_df['incidentBeginDate']).dt.year\n",
    "final_disasters_df['month'] = pd.to_datetime(final_disasters_df['incidentBeginDate']).dt.month\n",
    "\n",
    "# Ensure county is the correct type\n",
    "PRISM_precip['county'] = PRISM_precip['county'].astype(str).str.zfill(5)\n",
    "final_disasters_df['fullFIPS'] = final_disasters_df['fullFIPS'].astype(str).str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32672c05-442a-4106-a0de-0fd921fcabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Merge on year, month, and fullFIPS/county\n",
    "final_disasters_df = final_disasters_df.merge(\n",
    "    PRISM_precip[['year', 'month', 'county', 'PRISM_precipitation', 'PRISM_percentile']],\n",
    "    left_on=['year', 'month', 'fullFIPS'],  # Columns in final_disasters_df\n",
    "    right_on=['year', 'month', 'county'],  # Columns in PRISM_precip\n",
    "    how='left'  # Retain all rows from final_disasters_df\n",
    ")\n",
    "\n",
    "# Step 3: rename columns\n",
    "final_disasters_df.rename(columns={\n",
    "    'PRISM_precipitation': 'PRISM_mon_precipitation',\n",
    "    'PRISM_percentile': 'PRISM_mon_percentile'\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 4: Drop the temporary 'year' and 'month' columns if not needed further\n",
    "final_disasters_df.drop(['year', 'month', 'county'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8d292-0ea3-4a8f-8b55-d4f60e481dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSWEP Return Periods\n",
    "# First, convert MSWEP percentiles from 0-100 to 0-1 by dividing by 100\n",
    "final_disasters_df['returnPeriod_MSWEP_1d'] = final_disasters_df['MSWEP_precipitation_30d_max_1d_percentile_modeled'].apply(\n",
    "    lambda x: 1000 if x >= 99.9 else 1 / (1 - (x / 100))\n",
    ")\n",
    "final_disasters_df['returnPeriod_MSWEP_3d'] = final_disasters_df['MSWEP_precipitation_30d_max_3d_percentile_modeled'].apply(\n",
    "    lambda x: 1000 if x >= 99.9 else 1 / (1 - (x / 100))\n",
    ")\n",
    "final_disasters_df['returnPeriod_MSWEP_5d'] = final_disasters_df['MSWEP_precipitation_30d_max_5d_percentile_modeled'].apply(\n",
    "    lambda x: 1000 if x >= 99.9 else 1 / (1 - (x / 100))\n",
    ")\n",
    "final_disasters_df['returnPeriod_MSWEP_7d'] = final_disasters_df['MSWEP_precipitation_30d_max_7d_percentile_modeled'].apply(\n",
    "    lambda x: 1000 if x >= 99.9 else 1 / (1 - (x / 100))\n",
    ")\n",
    "final_disasters_df['returnPeriod_MSWEP_14d'] = final_disasters_df['MSWEP_precipitation_30d_max_14d_percentile_modeled'].apply(\n",
    "    lambda x: 1000 if x >= 99.9 else 1 / (1 - (x / 100))\n",
    ")\n",
    "final_disasters_df['returnPeriod_MSWEP_30d'] = final_disasters_df['MSWEP_precipitation_30d_sum_percentile_modeled'].apply(\n",
    "    lambda x: 1000 if x >= 99.9 else 1 / (1 - (x / 100))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a798373-c32f-4595-8105-9fb70d9a6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA5 Return Periods\n",
    "# First, convert ERA5 percentiles from 0-100 to 0-1 by dividing by 100\n",
    "final_disasters_df['returnPeriod_ERA5_1d'] = final_disasters_df['ERA5_precipitation_30d_max_1d_percentile_modeled'].apply(\n",
    "    lambda x: 1000 if x >= 99.9 else 1 / (1 - (x / 100))\n",
    ")\n",
    "final_disasters_df['returnPeriod_ERA5_3d'] = final_disasters_df['ERA5_precipitation_30d_max_3d_percentile_modeled'].apply(\n",
    "    lambda x: 1000 if x >= 99.9 else 1 / (1 - (x / 100))\n",
    ")\n",
    "final_disasters_df['returnPeriod_ERA5_5d'] = final_disasters_df['ERA5_precipitation_30d_max_5d_percentile_modeled'].apply(\n",
    "    lambda x: 1000 if x >= 99.9 else 1 / (1 - (x / 100))\n",
    ")\n",
    "final_disasters_df['returnPeriod_ERA5_7d'] = final_disasters_df['ERA5_precipitation_30d_max_7d_percentile_modeled'].apply(\n",
    "    lambda x: 1000 if x >= 99.9 else 1 / (1 - (x / 100))\n",
    ")\n",
    "final_disasters_df['returnPeriod_ERA5_14d'] = final_disasters_df['ERA5_precipitation_30d_max_14d_percentile_modeled'].apply(\n",
    "    lambda x: 1000 if x >= 99.9 else 1 / (1 - (x / 100))\n",
    ")\n",
    "final_disasters_df['returnPeriod_ERA5_30d'] = final_disasters_df['ERA5_precipitation_30d_sum_percentile_modeled'].apply(\n",
    "    lambda x: 1000 if x >= 99.9 else 1 / (1 - (x / 100))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67b8ef-9cce-4b60-a8ca-ef107d620b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRISM Return Periods\n",
    "final_disasters_df['returnPeriod_PRISM'] = final_disasters_df['PRISM_mon_percentile'].apply(\n",
    "    lambda x: 1000 if x >= 99.9 else 1 / (1 - (x / 100))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61055998-9bfc-46a0-9397-7fbf82420c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_disasters_df.to_csv('../../PRISM_MSWEP_ERA5_Processed_Disasters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a22e1-6cf7-4643-926a-b2193a1818d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_disasters_df = pd.read_csv('../../PRISM_MSWEP_ERA5_Processed_Disasters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d7f1b-79c7-4ae0-b0ae-527e871b0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of unfiltered records\n",
    "print(f\"{len(final_disasters_df)} unfiltered total records found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6b5a5-e480-427e-8e97-c9e658a76275",
   "metadata": {},
   "outputs": [],
   "source": [
    "if precipPresent:\n",
    "    # Group by fullFIPS and check if all values for precipitation_PRISM are 0\n",
    "    counties_with_all_zero_precip = final_disasters_df.groupby('fullFIPS').filter(\n",
    "        lambda x: (x['ERA5_precipitation_30d_sum'] == 0).all())\n",
    "    \n",
    "    # Get the count of rows per county where all values are zero\n",
    "    county_counts = counties_with_all_zero_precip['fullFIPS'].value_counts()\n",
    "    \n",
    "    # Print the counties and their counts\n",
    "    print(\"Counties where all values of precipitation are 0:\")\n",
    "    for county, count in county_counts.items():\n",
    "        print(f\"{county}: {count}\")\n",
    "    print(len(county_counts))\n",
    "\n",
    "    # Get the unique county codes to mask out\n",
    "    counties_to_mask_out = counties_with_all_zero_precip['fullFIPS'].unique()\n",
    "    \n",
    "    # Mask out these counties from final_disasters_df\n",
    "    final_disasters_df = final_disasters_df[~final_disasters_df['fullFIPS'].isin(counties_to_mask_out)]\n",
    "    \n",
    "    # Group by fullFIPS and check if all values for precipitation_PRISM are 0\n",
    "    counties_with_all_zero_precip = final_disasters_df.groupby('fullFIPS').filter(\n",
    "        lambda x: (x['MSWEP_precipitation_30d_sum'] == 0).all())\n",
    "    \n",
    "    # Get the count of rows per county where all values are zero\n",
    "    county_counts = counties_with_all_zero_precip['fullFIPS'].value_counts()\n",
    "    \n",
    "    # Print the counties and their counts\n",
    "    print(\"Counties where all values of precipitation are 0:\")\n",
    "    for county, count in county_counts.items():\n",
    "        print(f\"{county}: {count}\")\n",
    "    print(len(county_counts))\n",
    "\n",
    "    # Get the unique county codes to mask out\n",
    "    counties_to_mask_out = counties_with_all_zero_precip['fullFIPS'].unique()\n",
    "    \n",
    "    # Mask out these counties from final_disasters_df\n",
    "    final_disasters_df = final_disasters_df[~final_disasters_df['fullFIPS'].isin(counties_to_mask_out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fac7ce-cc50-4428-9db1-39c3f2888b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if monthlyMask:\n",
    "    total_records_before = len(final_disasters_df)\n",
    "    final_disasters_df = final_disasters_df[final_disasters_df['MSWEP_precipitation_30d_sum'].round(1) > mon_thres]\n",
    "    total_records_after = len(final_disasters_df)\n",
    "    records_filtered_out = total_records_before - total_records_after\n",
    "    total_records_before = total_records_after\n",
    "    print(f\"Total declarations filtered out under 30d MSWEP: {records_filtered_out}\")\n",
    "    final_disasters_df = final_disasters_df[final_disasters_df['ERA5_precipitation_30d_sum'].round(1) > mon_thres]\n",
    "    total_records_after = len(final_disasters_df)\n",
    "    records_filtered_out = total_records_before - total_records_after\n",
    "    total_records_before = total_records_after\n",
    "    print(f\"Total declarations filtered out under 30d ERA5: {records_filtered_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726669f0-6e34-4cdc-91b4-85bda5c4c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if percentileMask:\n",
    "    final_disasters_df = final_disasters_df[final_disasters_df['ERA5_precipitation_30d_max_7d'].round(1) > perc_thres]\n",
    "    total_records_after = len(final_disasters_df)\n",
    "    records_filtered_out = total_records_before - total_records_after\n",
    "    total_records_before = total_records_after\n",
    "    print(f\"Total declarations filtered out under 7d ERA5: {records_filtered_out}\")\n",
    "    total_records_before = len(final_disasters_df)\n",
    "    final_disasters_df = final_disasters_df[final_disasters_df['MSWEP_precipitation_30d_max_7d'].round(1) > perc_thres]\n",
    "    total_records_after = len(final_disasters_df)\n",
    "    records_filtered_out = total_records_before - total_records_after\n",
    "    total_records_before = total_records_after\n",
    "    print(f\"Total declarations filtered out under 7d MSWEP: {records_filtered_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515b452-cbb5-4b49-96c5-8f7c7b90dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of filtered records\n",
    "print(f\"{len(final_disasters_df)} filtered total records found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3ee3c-b66b-4353-a354-0686658470c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_disasters_df.to_csv('final_filtered_disasters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19230dc-32d9-487c-9c14-80d3877c3d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "if push:\n",
    "    !rm -r MSWEP_Daily_Precip_Processed_County\n",
    "    !rm -r ERA5_Daily_Precip_Processed_County\n",
    "    !rm 'PRISM_Monthly_Precip_Processed_County.csv'\n",
    "    # Note that we want to keep ERA5_Merged_Claims folder until all datasets are merged to the claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06658c-c04a-4a4f-abd2-63bde665830a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
